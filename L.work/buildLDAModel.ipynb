{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0       Id  AnswerCount             CreationDate  OwnerUserId  \\\n",
      "0           0  1786915            3  2009-11-23 23:54:33.523       178189   \n",
      "1           1  1786917            2  2009-11-23 23:55:14.417       160359   \n",
      "2           2  1786919            0  2009-11-23 23:55:34.790       102215   \n",
      "3           3     2809            5  2008-08-05 20:33:21.757          437   \n",
      "4           4    19995           18  2008-08-21 14:04:55.540          184   \n",
      "\n",
      "   Score  ViewCount                                               Text    neg  \\\n",
      "0      0       1379  i was wondering how people are handling the la...  0.010   \n",
      "1     55      14451  i have a project that adds some extensibility ...  0.000   \n",
      "2      1        397  i am a novice to eclipse cdt and its ui for cv...  0.058   \n",
      "3     11       2644  the table does not have a last updated field a...  0.075   \n",
      "4    134      73348  of course i am aware of ajax, but the problem ...  0.070   \n",
      "\n",
      "     neu    pos   Tags_y  \n",
      "0  0.870  0.120        c  \n",
      "1  0.968  0.032      NaN  \n",
      "2  0.890  0.051  eclipse  \n",
      "3  0.925  0.000      sql  \n",
      "4  0.853  0.076     ajax  \n"
     ]
    }
   ],
   "source": [
    "# Read the data being used to create the model\n",
    "df = pd.read_csv('FinalData/qdata.csv', encoding='utf-8')\n",
    "df = df[df['Text'].notna()]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "dict = df.to_dict('index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFirstLastThree(text):\n",
    "    text = text[3:]\n",
    "    text = text[:len(text)-3]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowerCase(text):\n",
    "    text = [word.lower() for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = []\n",
    "\n",
    "    for w in text:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)\n",
    "            \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPStemmer(text):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    stemmed_words=[]\n",
    "    for w in text:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lemma(text):\n",
    "    words = []\n",
    "    for word in text:\n",
    "        lemma = wn.morphy(word)\n",
    "        if (len(word) <= 2 or len(word) >= 15 or word == 'code' or word.isnumeric() or word == 'gt' or word == 'lt' or word =='quot' or word == 'pre' or word == 'amp'):\n",
    "            continue \n",
    "        elif lemma is None or word == lemma:\n",
    "            words.append(word)\n",
    "        else:\n",
    "            words.append(lemma)\n",
    "    return words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['peopl', 'handl', 'lack', 'enum', 'support', 'entiti', 'framework', 'deal', 'wcf', 'servic', 'common', 'practic', 'seem', 'declar', 'getter', 'setter', 'entiti', 'properti', 'privat', 'creat', 'partial', 'class', 'entiti', 'declar', 'enum', 'properti', 'fine', 'dandi', 'pose', 'issu', 'wcf', 'servic', 'need', 'enum', 'definit', 'client', 'side', 'want', 'happen', 'automat', 'proxi', 'gener', 'add', 'datamemb', 'attribut', 'enum', 'properti', 'share', 'class', 'includ', 'serial', 'process', 'howev', 'call', 'wcf', 'method', 'serial', 'int', 'enum', 'properti', 'thu', 'carri', 'redund', 'data', 'increas', 'messag', 'size', 'best', 'way', 'deal', 'peopl', 'handl', 'scenario']\n",
      "['project', 'add', 'extens', 'anoth', 'applic', 'api', 'howev', 'want', 'abl', 'use', 'project', 'multipl', 'version', 'applic', 'howev', 'version', 'applic', 'requir', 'refer', 'proper', 'assembl', 'version', 'softwar', 'load', 'assembl', 'gac', 'even', 'could', 'specifi', 'version', 'assembl', 'use', 'base', 'build', 'configur', 'would', 'fine', 'way', 'insid', 'need', 'extern']\n",
      "['novic', 'eclips', 'cdt', 'command', 'line', 'releas', 'undo', 'effect', 'checkout', 'releas', 'checkout', 'eclips', 'cdt', 'thank', 'edit', 'guess', 'could', 'like', 'eclips', 'delet', 'project', 'shell', 'run', 'releas', 'workspac', 'project', 'workspac', 'project', 'resort', 'command', 'line', 'nice']\n",
      "['last', 'updat', 'field', 'need', 'know', 'exist', 'data', 'updat', 'last', 'updat', 'field', 'help', 'far']\n",
      "['awar', 'ajax', 'problem', 'ajax', 'browser', 'poll', 'server', 'frequent', 'find', 'whether', 'new', 'data', 'increas', 'server', 'load', 'better', 'method', 'even', 'use', 'ajax', 'poll']\n"
     ]
    }
   ],
   "source": [
    "# Process LDA model\n",
    "text_tokens = []\n",
    "i = 0\n",
    "for key in dict:\n",
    "    tokens = tokenizeText(dict[key]['Text'])\n",
    "    tokens = removeFirstLastThree(tokens)\n",
    "    tokens = toLowerCase(tokens)\n",
    "    tokens = removeStopWords(tokens)\n",
    "    tokens = applyPStemmer(tokens)\n",
    "    tokens = get_lemma(tokens)\n",
    "\n",
    "    \n",
    "\n",
    "    if i < 5:\n",
    "        print(tokens)\n",
    "    text_tokens.append(tokens)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(133388 unique tokens: ['add', 'attribut', 'automat', 'best', 'call']...)\n"
     ]
    }
   ],
   "source": [
    "dict = corpora.Dictionary(text_tokens)\n",
    "dict.save('dictionary.gensim')\n",
    "\n",
    "bagOfWords = [dict.doc2bow(t) for t in text_tokens]\n",
    "pickle.dump(bagOfWords, open('models/corpus.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 25\n",
    "NUM_WORDS = 2\n",
    "\n",
    "# Commenting to avoid overwriting existing saved model\n",
    "# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "# ldamodel.save('models/lda_model.model')\n",
    "\n",
    "ldamodel =  models.LdaModel.load('models/lda_model.model')\n",
    "topics = ldamodel.print_topics(num_words = NUM_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, '0.192*\"self\" + 0.026*\"let\"')\n",
      "(8, '0.118*\"file\" + 0.038*\"line\"')\n",
      "(17, '0.115*\"image\" + 0.046*\"category\"')\n",
      "(16, '0.107*\"div\" + 0.103*\"class\"')\n",
      "(10, '0.084*\"string\" + 0.075*\"public\"')\n",
      "(22, '0.058*\"int\" + 0.034*\"array\"')\n",
      "(23, '0.237*\"app\" + 0.076*\"tag\"')\n",
      "(15, '0.037*\"version\" + 0.030*\"build\"')\n",
      "(14, '0.139*\"property\" + 0.047*\"binding\"')\n",
      "(7, '0.067*\"function\" + 0.057*\"var\"')\n",
      "(24, '0.022*\"strong\" + 0.019*\"like\"')\n",
      "(19, '0.102*\"http\" + 0.073*\"com\"')\n",
      "(18, '0.072*\"stack\" + 0.070*\"http\"')\n",
      "(4, '0.032*\"server\" + 0.028*\"test\"')\n",
      "(11, '0.192*\"android\" + 0.049*\"view\"')\n",
      "(21, '0.115*\"model\" + 0.079*\"date\"')\n",
      "(9, '0.033*\"task\" + 0.032*\"system\"')\n",
      "(6, '0.048*\"row\" + 0.046*\"list\"')\n",
      "(13, '0.066*\"product\" + 0.059*\"query\"')\n",
      "(1, '0.065*\"width\" + 0.063*\"color\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     Id  AnswerCount             CreationDate  OwnerUserId  \\\n",
      "4           4  19995           18  2008-08-21 14:04:55.540          184   \n",
      "\n",
      "   Score  ViewCount                                               Text   neg  \\\n",
      "4    134      73348  of course i am aware of ajax, but the problem ...  0.07   \n",
      "\n",
      "     neu    pos Tags_y  \n",
      "4  0.853  0.076   ajax  \n",
      "['video', 'redirect', 'back', 'homepag']\n",
      "[(0, 0.010001341), (1, 0.010001341), (2, 0.010001341), (3, 0.010001341), (4, 0.010001341), (5, 0.010001341), (6, 0.010001341), (7, 0.010001341), (8, 0.010001341), (9, 0.010001341), (10, 0.010001341), (11, 0.010001341), (12, 0.010001341), (13, 0.010001341), (14, 0.010001341), (15, 0.010001341), (16, 0.010001341), (17, 0.010001341), (18, 0.010001341), (19, 0.010001341), (20, 0.010001341), (21, 0.010001341), (22, 0.42947686), (23, 0.010001341), (24, 0.3404923)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = df.iloc[[4]]\n",
    "print(new_doc)\n",
    "\n",
    "text = 'when i click on a video, it is redirecting back to homepage, youtube clone'\n",
    "\n",
    "\n",
    "# tokens = prepare_text_for_lda(dict[key]['Body'])\n",
    "tokens = tokenizeText(text)\n",
    "tokens = removeFirstLastThree(tokens)\n",
    "tokens = toLowerCase(tokens)\n",
    "tokens = removeStopWords(tokens)\n",
    "tokens = applyPStemmer(tokens)\n",
    "tokens = get_lemma(tokens)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "new_doc_bow = dict.doc2bow(tokens)\n",
    "# print(new_doc_bow)\n",
    "# print(ldamodel.get_document_topics(new_doc_bow))\n",
    "# new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "# print(new_doc_bow)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.010001341), (1, 0.010001341), (2, 0.010001341), (3, 0.010001341), (4, 0.010001341), (5, 0.010001341), (6, 0.010001341), (7, 0.010001341), (8, 0.010001341), (9, 0.010001341), (10, 0.010001341), (11, 0.010001341), (12, 0.010001341), (13, 0.010001341), (14, 0.010001341), (15, 0.010001341), (16, 0.010001341), (17, 0.010001341), (18, 0.010001341), (19, 0.010001341), (20, 0.010001341), (21, 0.010001341), (22, 0.4296849), (23, 0.010001341), (24, 0.34028426)]\n",
      "\n",
      "\n",
      "(19, '0.102*\"http\" + 0.073*\"com\"')\n",
      "(18, '0.072*\"stack\" + 0.070*\"http\"')\n",
      "(17, '0.115*\"image\" + 0.046*\"category\"')\n",
      "(1, '0.065*\"width\" + 0.063*\"color\"')\n",
      "(21, '0.115*\"model\" + 0.079*\"date\"')\n",
      "(9, '0.033*\"task\" + 0.032*\"system\"')\n",
      "(16, '0.107*\"div\" + 0.103*\"class\"')\n",
      "(8, '0.118*\"file\" + 0.038*\"line\"')\n",
      "(20, '0.057*\"user\" + 0.028*\"request\"')\n",
      "(4, '0.032*\"server\" + 0.028*\"test\"')\n",
      "(23, '0.237*\"app\" + 0.076*\"tag\"')\n",
      "(2, '0.091*\"name\" + 0.062*\"type\"')\n",
      "(0, '0.100*\"php\" + 0.074*\"lib\"')\n",
      "(10, '0.084*\"string\" + 0.075*\"public\"')\n",
      "(22, '0.058*\"int\" + 0.034*\"array\"')\n",
      "(14, '0.139*\"property\" + 0.047*\"binding\"')\n",
      "(11, '0.192*\"android\" + 0.049*\"view\"')\n",
      "(24, '0.022*\"strong\" + 0.019*\"like\"')\n",
      "(15, '0.037*\"version\" + 0.030*\"build\"')\n",
      "(5, '0.041*\"event\" + 0.034*\"child\"')\n"
     ]
    }
   ],
   "source": [
    "loaded_model = gensim.models.ldamodel.LdaModel.load('models/lda_model.model')\n",
    "print(loaded_model.get_document_topics(new_doc_bow))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "topics = loaded_model.print_topics(num_words=2)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f67f7bbca0598e70e2d2a4cdc15d594b3c82a687c6c3ba15a4a1d5e50cae4bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
