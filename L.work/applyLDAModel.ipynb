{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file responds to a post request from app.py \n",
    "# Returns the 3 closest topics based on the passed in text\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_gensim_model = gensim.models.ldamodel.LdaModel.load('models/lda_model.model')\n",
    "topics = load_gensim_model.print_topics(num_words=2)\n",
    "\n",
    "file_to_read = open(\"models/dictionary.gensim\", \"rb\")\n",
    "dictionary = pickle.load(file_to_read)\n",
    "\n",
    "def loadModels():\n",
    "    word_pairs = []\n",
    "    word_pairs_cleaned = []\n",
    "\n",
    "    for topic in topics:\n",
    "        word_pairs.append(list(topic))\n",
    "\n",
    "    word_pairs_sorted = sorted(word_pairs, key = lambda x: x[0])\n",
    "\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "    for element in word_pairs_sorted:\n",
    "        word_pairs = regex.sub('', element[1])\n",
    "        word_pairs = word_pairs.replace('  ', ', ')\n",
    "        word_pairs_cleaned.append([element[0], word_pairs])\n",
    "\n",
    "    return word_pairs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getKeywords(word_pairs_cleaned):\n",
    "    list_of_keywords = [None] * 25\n",
    "\n",
    "    list_of_keywords[0] = ''\n",
    "    list_of_keywords[2] = ''\n",
    "    list_of_keywords[3] = ''\n",
    "    list_of_keywords[5] = ''\n",
    "\n",
    "    list_of_keywords[1] = [word_pairs_cleaned[0], 'CSS Styling']\n",
    "    list_of_keywords[4] = [word_pairs_cleaned[1], 'Server testing']\n",
    "    list_of_keywords[6] = [word_pairs_cleaned[2], 'Arrays and lists']\n",
    "    list_of_keywords[7] = [word_pairs_cleaned[3], 'Functions, methods and variables']\n",
    "    list_of_keywords[8] = [word_pairs_cleaned[4], 'Files']\n",
    "    list_of_keywords[9] = [word_pairs_cleaned[5], 'System task management']\n",
    "    list_of_keywords[10] = [word_pairs_cleaned[6], 'JAVA developement']\n",
    "    list_of_keywords[11] = [word_pairs_cleaned[7], 'Android view class']\n",
    "    list_of_keywords[12] = [word_pairs_cleaned[8], 'Accessing and declaring variables']\n",
    "    list_of_keywords[13] = [word_pairs_cleaned[9], 'Database queries']\n",
    "    list_of_keywords[14] = [word_pairs_cleaned[10], 'Static and dynamic binding']\n",
    "    list_of_keywords[15] = [word_pairs_cleaned[11], 'Verson and builds']\n",
    "    list_of_keywords[16] = [word_pairs_cleaned[12], 'HTML elements']\n",
    "    list_of_keywords[17] = [word_pairs_cleaned[13], 'Types of images']\n",
    "    list_of_keywords[18] = [word_pairs_cleaned[14], 'Protocol/network stacks']\n",
    "    list_of_keywords[19] = [word_pairs_cleaned[15], 'HTTP and TCP/IP']\n",
    "    list_of_keywords[21] = [word_pairs_cleaned[16], 'Software models and development life cycle']\n",
    "    list_of_keywords[22] = [word_pairs_cleaned[17], 'Primitive and reference data types']\n",
    "    list_of_keywords[23] = [word_pairs_cleaned[18], 'Application tags and metadata']\n",
    "    list_of_keywords[24] = [word_pairs_cleaned[19], 'HTML elements']\n",
    "\n",
    "    return list_of_keywords\n",
    "\n",
    "def tokenizeText(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def removeFirstLastThree(text):\n",
    "    text = text[3:]\n",
    "    text = text[:len(text)-3]\n",
    "    return text\n",
    "\n",
    "\n",
    "def toLowerCase(text):\n",
    "    text = [word.lower() for word in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "def removeStopWords(text):\n",
    "    stop_words= set(stopwords.words(\"english\"))\n",
    "    filtered_sent = []\n",
    "    for w in text:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(w)\n",
    "    return filtered_sent\n",
    "\n",
    "\n",
    "\n",
    "def applyPStemmer(text):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    stemmed_words = []\n",
    "    for w in text:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "def get_lemma(text):\n",
    "    words = []\n",
    "    for word in text:\n",
    "        lemma = wn.morphy(word)\n",
    "        if (len(word) <= 2 or len(word) >= 15 or word == 'code' or word.isnumeric() or word == 'gt' or word == 'lt' or word =='quot' or word == 'pre' or word == 'amp'):\n",
    "            continue \n",
    "        elif lemma is None or word == lemma:\n",
    "            words.append(word)\n",
    "        else:\n",
    "            words.append(lemma)\n",
    "    return words\n",
    "\n",
    "list_of_keywords = []\n",
    "\n",
    "# Used with strings\n",
    "def getTopics(user_input):\n",
    "    tokens = tokenizeText(user_input)\n",
    "    tokens = removeFirstLastThree(tokens)\n",
    "    tokens = toLowerCase(tokens)\n",
    "    tokens = removeStopWords(tokens)\n",
    "    tokens = applyPStemmer(tokens)\n",
    "    tokens = get_lemma(tokens)\n",
    "\n",
    "    bagOfWords = dictionary.doc2bow(tokens)\n",
    "    matches = load_gensim_model.get_document_topics(bagOfWords)\n",
    "\n",
    "    word_pairs_cleaned = loadModels()\n",
    "    list_of_keywords = getKeywords(word_pairs_cleaned)\n",
    "\n",
    "    matches.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    matches = matches[:3]\n",
    "\n",
    "    to_return = []\n",
    "    \n",
    "    for match in matches:\n",
    "        temp = list(match)\n",
    "        if(temp[0]):\n",
    "            if(match[1] > 0.1):\n",
    "                to_return.append(list_of_keywords[match[0]][1])\n",
    "\n",
    "\n",
    "    return to_return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for handling dataframes\n",
    "def prepareTextDf(df):\n",
    "\n",
    "    df['Body_processed'] = df['Text'].astype(str)\n",
    "    df['Body_processed'] = df.apply(lambda x: removeFirstLastThree(x['Body_processed']), axis=1)\n",
    "    df['Body_processed'] = df.apply(lambda x: tokenizeText(x['Body_processed']), axis=1)\n",
    "    df['Body_processed'] = df.apply(lambda x: applyPStemmer(x['Body_processed']), axis=1)\n",
    "    df['Body_processed'] = df.apply(lambda x: removeStopWords(x['Body_processed']), axis=1)\n",
    "    df['Body_processed'] = df.apply(lambda x: get_lemma(x['Body_processed']), axis=1)\n",
    "\n",
    "    word_pairs_cleaned = loadModels()\n",
    "    list_of_keywords = getKeywords(word_pairs_cleaned)\n",
    "\n",
    "    df['Body_processed_topics'] = df.apply(lambda x: dictionary.doc2bow(x['Body_processed']), axis=1)\n",
    "    df['Body_processed_topics'] = df.apply(lambda x: load_gensim_model.get_document_topics(x['Body_processed_topics']), axis=1)\n",
    "\n",
    "\n",
    "    def sorterTopThree(nums):\n",
    "        nums.sort(key = lambda x: x[1], reverse = True)\n",
    "        return nums[:3]\n",
    "\n",
    "    df['Body_processed_topics_sorted'] = df.apply(lambda x: sorterTopThree(x['Body_processed_topics']), axis=1)\n",
    "\n",
    "\n",
    "    def getTopicsfromKwrds(keywords):\n",
    "        to_return = []\n",
    "        i = 0\n",
    "        for match in keywords:\n",
    "            temp = list(match)\n",
    "            if(list_of_keywords[temp[0]]):\n",
    "                    if(temp[1] > 0.1):\n",
    "                        to_return.append(list_of_keywords[temp[0]][1])\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        return to_return\n",
    "\n",
    "\n",
    "    df['Body_processed_topics_words_final'] = df.apply(lambda x: getTopicsfromKwrds(x['Body_processed_topics_sorted']), axis=1)\n",
    "    df = df.drop(labels=['Body_processed_topics', 'Body_processed_topics_sorted'], axis=1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getTopics('Below is an example of how you can instruct your audience on installing and setting up your app. This template doesn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FinalData/qdata.csv', encoding='utf-8')\n",
    "print(prepareTextDf(df.head(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA model to each row in body and store results as a new column\n",
    "qdataCombined = pd.read_csv(\"FinalData\\\\qdata.csv\")\n",
    "\n",
    "qdataCombined = prepareTextDf(qdataCombined.head(100))\n",
    "print(qdataCombined['Body_processed_topics_words_final'])\n",
    "\n",
    "# qdataCombined.to_csv(\"FinalData\\\\qdataCombined.csv\", encoding=\"utf-8\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8aaefb19c644fc8858c30316f1bc7fd3b4dc4f1282d6c4193edf1f6bbd1bd67c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
